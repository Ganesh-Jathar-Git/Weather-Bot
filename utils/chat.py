import os
from dotenv import load_dotenv
from ibm_watsonx_ai import Credentials
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.foundation_models.schema import TextChatParameters
load_dotenv()

# Use of environment variables
API_KEY=os.getenv("IBM_API_KEY")
PROJECT_ID=os.getenv("IBM_PROJECT_ID")
MODEL_ID=os.getenv("MODEL_ID")
URL=os.getenv("URL")

# Set up IBM watsonx credentials
credentials = Credentials(
    url=URL,
    api_key=API_KEY,
)

#Function to generatate the suggestion on the basis of weather condition.
def generate_suggestion(text, weather):
    prompt = f"""
            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€:ã€Œ{text}ã€
            ç¾åœ¨ã®å¤©æ°—æƒ…å ±: {weather}

        ã“ã®æƒ…å ±ã«åŸºã¥ã„ã¦ã€ä»¥ä¸‹ã®ä¸­ã‹ã‚‰æœ€å¤§3ã¤ã€100æ–‡å­—ä»¥å†…ã§è¦ªã—ã¿ã‚„ã™ãå®Ÿç”¨çš„ãªææ¡ˆã‚’Markdownå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚

        ã€è¦–ç‚¹ã®ä¾‹ã€‘
        ãƒ»æ—…è¡Œã‚„ãŠã§ã‹ã‘ã®ãŠã™ã™ã‚
        ãƒ»ä»Šæ—¥ã®ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³
        ãƒ»æ°—åˆ†ã«åˆã†éŸ³æ¥½
        ãƒ»å±‹å¤–ä½œæ¥­ã‚„é‹å‹•ã®ãƒ’ãƒ³ãƒˆ

        ã€æ¡ä»¶ã€‘
        - å‡ºåŠ›ã¯Markdownå½¢å¼ã§ã€**ç®‡æ¡æ›¸ãã®ã¿**ï¼ˆ*ã‚„ãƒ»ï¼‰ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
        - ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã‚„å¤©æ°—æƒ…å ±ã¯**å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„**ã€‚
        - å†…å®¹ã«å¿œã˜ã¦é©åˆ‡ãªçµµæ–‡å­—ï¼ˆğŸŒ‚ğŸ‘•â˜€ï¸ãªã©ï¼‰ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚
        - ä¸å¯§ã§ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ—¥æœ¬èªã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
        """

    params = TextChatParameters(temperature=0.7)
    model = ModelInference(
        model_id=MODEL_ID,
        credentials=credentials,
        project_id=PROJECT_ID,
        params=params,
    )

    messages = [
        {
            "role": "system",
            "content": "ã‚ãªãŸã¯æ—¥æœ¬èªã§ä¸å¯§ã«å¯¾å¿œã™ã‚‹å¤©æ°—ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚å¤©æ°—ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¨€è‘‰ã‹ã‚‰ã€å½¹ç«‹ã¤ã‚¢ã‚¤ãƒ‡ã‚¢ã‚„ææ¡ˆã‚’ã—ã¾ã™ã€‚æ—…è¡Œã€éŸ³æ¥½ã€è¾²æ¥­ã€ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ã€ã‚¹ãƒãƒ¼ãƒ„ãªã©ã€è‡ªç”±ã«ç™ºæƒ³ã—ã¦ãã ã•ã„.",
        },
        {
            "role": "user",
            "content": prompt,
        },
    ]

    try:
        response = model.chat(messages=messages)
        return response["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"


#Function to extract the City from transcript.
def extract_city(transcript: str) -> str:
    prompt = f"""
ä»¥ä¸‹ã®æ—¥æœ¬èªã®ç™ºè©±ã‹ã‚‰ã€è©±ã—æ‰‹ãŒè¨€åŠã—ã¦ã„ã‚‹éƒ½å¸‚ã¾ãŸã¯åœ°åŸŸåã‚’ä¸€ã¤ã ã‘æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ç™ºè©±ã«è¤‡æ•°ã®åœ°åãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€æœ€ã‚‚é–¢é€£æ€§ãŒé«˜ã„ã‚‚ã®ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
å­˜åœ¨ã—ãªã„å ´åˆã¯ã€Œä¸æ˜ã€ã¨è¿”ã—ã¦ãã ã•ã„ã€‚

ç™ºè©±: ã€Œ{transcript}ã€
å‡ºåŠ›ï¼ˆéƒ½å¸‚åã®ã¿ï¼‰:
"""
    params = TextChatParameters(temperature=0.0)
    model = ModelInference(
        model_id=MODEL_ID,
        credentials=credentials,
        project_id=PROJECT_ID,
        params=params,
    )

    messages = [
        {
            "role": "system",
            "content": (
                "ã‚ãªãŸã¯æ—¥æœ¬èªã®ç™ºè©±ã‹ã‚‰éƒ½å¸‚ã¾ãŸã¯åœ°åŸŸåã‚’æ­£ç¢ºã«æŠ½å‡ºã™ã‚‹AIã§ã™ã€‚"
                "å¿…ãš1ã¤ã ã‘ã€æ—¥æœ¬èªã§éƒ½å¸‚åã¾ãŸã¯åœ°åŸŸåã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
                "éƒ½å¸‚ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€Œä¸æ˜ã€ã¨è¿”ã—ã¦ãã ã•ã„ã€‚"
            ),
        },
        {
            "role": "user",
            "content": prompt,
        },
    ]

    try:
        response = model.chat(messages=messages)
        city = response["choices"][0]["message"]["content"].strip()

        # Checking the ambigious values
        if city in ["ä¸æ˜", "", "ãªã—", "ç„¡ã—"]:
            return "æ±äº¬"  # fallback city
        return city

    except Exception as e:
        return city if city != "ä¸æ˜" and city != "" else "æ±äº¬"  # fallback on failure



